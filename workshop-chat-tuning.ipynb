{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456b0ba-e623-4f27-b820-a65a6bbd4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37b2cb-4ff6-4ed9-af5e-c89d245b3f7d",
   "metadata": {},
   "source": [
    "# Build a Chat Model Using Fine-tuning\n",
    "\n",
    "Conversation fine-tuning builds on top of instruction tuning to make the model be even better at continued conversation with a human. Chat models are created using this techinque.\n",
    "\n",
    "In a conversation each dialog comes from an actor with a well defined role. Example conversation:\n",
    "\n",
    "```\n",
    "User: When was Abraham Lincoln born?\n",
    "LLM: Abraham Lincoln was born on February 12, 1809.\n",
    "\n",
    "User: How old was he when he died?\n",
    "LLM: Abraham Lincoln died on April 15, 1865, at the age of 56.\n",
    "\n",
    "User: Where did he die?\n",
    "LLM: Abraham Lincoln was assasinated in Washington D.C.\n",
    "```\n",
    "\n",
    "We will now fine-tuned the base model ``TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T`` for conversation. We will adopt the following prompt syntax to designate the roles for the dialog.\n",
    "\n",
    "```\n",
    "<|user|>\n",
    "When was Abraham Lincoln born?</s> \n",
    "<|assistant|>\n",
    "Abraham Lincoln was born on February 12, 1809.</s> \n",
    "<|user|>\n",
    "How old was he when he died?</s> \n",
    "<|assistant|>\n",
    "Abraham Lincoln died on April 15, 1865, at the age of 56.</s> \n",
    "<|user|>\n",
    "Where did he die?</s> \n",
    "<|assistant|>\n",
    "Abraham Lincoln was assasinated in Washington D.C.</s>\n",
    "```\n",
    "\n",
    "## Prepare Training Data\n",
    "\n",
    "We will use [tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) to train the model.\n",
    "\n",
    "SFTTrainer supports [two different data formats](https://huggingface.co/docs/trl/en/sft_trainer#dataset-format-support). We will use the conversation format where each piece of training data will be as follows.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who was Ada Lovelace?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Ada Lovelace was a Mathematician.\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "These columns in the source dataset are of interest to us:\n",
    "\n",
    "- instruction – User question\n",
    "- input - Any additional context about the question\n",
    "- output – Desired response from the model\n",
    "\n",
    "Let's look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa84e6-5e64-4f44-985e-7c9799d6a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"medalpaca/medical_meadow_medical_flashcards\",\n",
    "    split=\"train\"\n",
    ").train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224bf500-7905-4d88-858d-23d8b60a308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669eaa34-aa98-47b5-87b7-58e451d63e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12986eb1-e2ea-4825-89c4-5608849c0275",
   "metadata": {},
   "source": [
    "The code below will load the dataset, reformat it according to the requirement of SFTTrainer and save it in the ``instruction_dataset.jsonl`` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e74a3f-4617-4f97-8947-391bd70427ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset):\n",
    " \n",
    "    #Data mapping function\n",
    "    def create_conversation(sample):   \n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are medical professional. Answer the question with most scientific accuracy.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": sample[\"input\"]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": sample[\"output\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    #By default the map() function merges new columns to the dataset.\n",
    "    dataset = dataset.map(\n",
    "        create_conversation, \n",
    "        remove_columns=[\"input\", \"output\", \"instruction\"])\n",
    "\n",
    "    # Save dataset\n",
    "    dataset[\"train\"].to_json(\"train_medical_dataset.jsonl\", orient=\"records\")\n",
    "    dataset[\"test\"].to_json(\"test_medical_dataset.jsonl\", orient=\"records\")\n",
    " \n",
    "prepare_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0019cb-074d-4a88-9805-6f508bee8e00",
   "metadata": {},
   "source": [
    "JSONL is an interesting format where each line is a JSON document. Open the ``train_dataset.jsonl`` file and review it.\n",
    "\n",
    "Data conversion needs to be done only once. Before running training we need to load the converted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e474eab-20bc-43b3-9665-f53e4fbef879",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\n",
    "    \"json\", \n",
    "    data_files=\"train_medical_dataset.jsonl\", \n",
    "    split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3fa8d0-39a5-4ec8-86ab-d7ea5c662796",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2cfeb-7c48-4b0e-b6c0-329981d81303",
   "metadata": {},
   "source": [
    "## Load the Base Model\n",
    "\n",
    "This code will load the base model with 4bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c5e0d-26c0-4d1a-b6d0-c3b0df77f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    #For 4bit quantization\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "base_model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name)\n",
    "\n",
    "#The base tokenizer does not have a prompt template.\n",
    "#We add it here.\n",
    "tokenizer.chat_template = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831520e7-3770-4d78-a6f3-43c4a4b71434",
   "metadata": {},
   "source": [
    "## Evaluate the Base Model\n",
    "\n",
    "Before running any training we should see if the base model is any good at solving our problems. We write a simple utility to perform text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30182042-9315-4ebc-88b8-ffcfed50f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, question):\n",
    "  streamer = TextStreamer(tokenizer)\n",
    "  \n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are medical professional. Answer the question with most scientific accuracy.\"},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "  ]\n",
    "\n",
    "  #This will convert the messages list to text and then tokenize it.\n",
    "  encoded = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      add_generation_prompt=True,\n",
    "      return_tensors=\"pt\").to(model.device)\n",
    " \n",
    "  generated_ids = model.generate(encoded, streamer=streamer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d0d22-ef1d-4df2-981a-d9b107ecfe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give it a try.\n",
    "generate(base_model, tokenizer, \"What is the name of the active form of vitamin D?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63048a54-2003-4dfb-9729-002aada0412c",
   "metadata": {},
   "source": [
    "Biggest problem with the model right now is that it doesn't know when to stop answering. Let’s see if fine-tuning will help.\n",
    "\n",
    "## Run Training\n",
    "\n",
    "First we configure the training parameters. We run training for 1 epoch. Each batch will have 5 samples of training data. We set the maximum sequence length to only 500 because we're using a very small language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb811a35-467e-43b5-994f-7f5f061ff0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    " \n",
    "args = SFTConfig(\n",
    "    output_dir=\"medical-trained-model\", # directory to save and repository id\n",
    "    num_train_epochs=1,                     # number of training epochs\n",
    "    per_device_train_batch_size=5,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=2,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    bf16=False,                              # use bfloat16 precision\n",
    "    tf32=False,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    max_length=500, #Maximum number of generated tokens\n",
    "    packing=True,\n",
    ")\n",
    " \n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f22649-30ee-4db4-a830-1646d390b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = trainer.get_train_dataloader()\n",
    "first_batch = next(iter(train_dataloader))\n",
    "# print(first_batch['input_ids'][0])\n",
    "\n",
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299b71a-40ed-4adb-a61c-0f1caac9b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(first_batch[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80294cb1-0a94-4906-af28-d557c079907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_batch = first_batch[\"input_ids\"] \n",
    "decoded_texts = [tokenizer.decode(input_ids, skip_special_tokens=False) for input_ids in input_ids_batch]\n",
    "print(decoded_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81265900-1604-4246-a2cc-3a41bb254a2d",
   "metadata": {},
   "source": [
    "Now, we can begin training. As training progresses you should see a dramatic reduction in loss. This is always a welcome sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b094c-aaf8-42b8-abb0-d83bf050e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aba3c7-f088-463a-955c-96c811d35995",
   "metadata": {},
   "source": [
    "While training is going on, you can use the ``nvidia-smi`` command to check GPU usage and memory avalability.\n",
    "\n",
    "## Save the Model\n",
    "\n",
    "The model weights are saved for every epoch in the ``./chat-trained-model`` folder. But we should save the final version. This will save the model as well as the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d2520-fcbc-4425-99c8-15a993eb46ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97eb05-8651-4887-904d-9e22222d0d6c",
   "metadata": {},
   "source": [
    "Open ``./medical-trained-model/tokenizer_config.json`` to verify that the chat template is now set for the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9256b511-9df8-4279-a532-19689b3bd4af",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "To run inference we need to load the fine-tuned model from the ``./trained-model`` folder. This model is already quantized. There’s no need to quantize it again.\n",
    "\n",
    "Before you go forward I recommend that you restart the notebook session or run this code to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde439ce-1f28-445e-88bf-7c3a12cf50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Free up memory taken up during training\n",
    "del base_model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45105985-746f-496f-bb7f-9f7a7ffb8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"medical-trained-model\",\n",
    "    device_map=\"auto\")\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"medical-trained-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb88ffe-092e-4324-bf03-0482c87013b9",
   "metadata": {},
   "source": [
    "Run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acc5e0f-0af7-4d39-b1b5-009f6bc01c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model, tokenizer, \"What is transformation and how is it characterized as the direct uptake of naked DNA by bacteria?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c033d27a-b414-4f6f-9cdd-bcb9d33bd78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model, tokenizer, \"Which acid-base disturbance is a normal physiological change during pregnancy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45df40-248a-420a-b9cb-1ee4eb20242c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here we built a proper chat model. Applications can now supply a chat history and get a relevant response from the model. An example below. Notice how ``\"he\"`` in the last user prompt is correlated by the model to ``Abraham Lincoln``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b9e1e-7a9a-43db-9514-a21062c20027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
