{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456b0ba-e623-4f27-b820-a65a6bbd4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37b2cb-4ff6-4ed9-af5e-c89d245b3f7d",
   "metadata": {},
   "source": [
    "# Workshop - Build a Medical Chat Model\n",
    "\n",
    "You will fine-tune a pre-trained model (``\"EleutherAI/pythia-1B-deduped\"``) to be able to answer medical questions. The base model has very little medical knowledge and has no chatting capability. The base model can only predict next token.\n",
    "\n",
    "## Load the Dataset\n",
    "Load the ``\"medalpaca/medical_meadow_medical_flashcards\"`` dataset. Use only 1000 samples to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "808c2816-0a42-4448-90a3-955a876fddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "\n",
    "#Use only 1000 samples\n",
    "\n",
    "#Inspect the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7a64d-8bb1-42b7-8d20-ed0fdf9ada20",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "Map the dataset to the conversational format required by ``SFTTrainer``. Save the transformed dataset to ``train_medical_dataset.jsonl`` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037cc65-5b8c-44e9-aeb8-5a182ad670d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map and save dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c34bd-d72a-4f3d-9837-9690f6e26adb",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "Load the transformed dataset from ``train_medical_dataset.jsonl``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273babd7-54ac-4fd9-97d4-cf6466a47e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b2df8-c7e8-4f9c-baa9-5007d8f3b914",
   "metadata": {},
   "source": [
    "## Load the Base Model\n",
    "\n",
    "Load the base model ``\"EleutherAI/pythia-1B-deduped\")`` with 4bit quantization. Also load the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb338936-b481-41ae-a8c8-7ab38a746ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load base model with 4bit quantization\n",
    "\n",
    "#Load the tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab633a-dbdd-467a-85cb-551b3dbff979",
   "metadata": {},
   "source": [
    "## Set Chat Template\n",
    "The base tokenizer doesn't have any chat template. Set the same template that was used in the ``fine-tune-chat-template.ipynb`` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ea5f2-4979-4ea2-97d8-abac8fdbce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set chat template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe8544-3ed4-41f2-a4f0-2a8757299089",
   "metadata": {},
   "source": [
    "## Evaluate the Base Model\n",
    "Inspect how the base model performs when you ask it a few questions from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95394437-4e9a-49cb-beca-6612cdd4a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that will generate answer from a question\n",
    "# Set max_new_tokens to 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6f17c-d473-4c89-919d-d99438ab4492",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "Run LoRA training using these parameters: \n",
    "\n",
    "- 2 epochs. \n",
    "- Batch size 5.\n",
    "- Maximum sequence length to only 300 because we're using a very small language model.\n",
    "- Model save directory ``\"medical-trained-model\"``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35578171-a522-4ff4-8e5e-c6ef4e24a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deeb2fe-ca23-4edc-92b8-42765bf8f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e451f006-0631-431f-b371-ebc88776068e",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "Load the trained model and tokenizer and run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05727cd7-cf92-4100-abdd-ac8fe5d6c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load trained model\n",
    "\n",
    "#Load trained tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe62c4-5413-468c-8365-e3ed608573a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edc5501-8b27-4b1a-83da-0b39682431fd",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa84e6-5e64-4f44-985e-7c9799d6a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224bf500-7905-4d88-858d-23d8b60a308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use only 1000 samples to speed up training\n",
    "dataset = dataset[\"train\"].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669eaa34-aa98-47b5-87b7-58e451d63e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12986eb1-e2ea-4825-89c4-5608849c0275",
   "metadata": {},
   "source": [
    "## Prepare Dataset for Training\n",
    "The code below will load the dataset, reformat it according to the requirement of SFTTrainer and save it in the ``train_medical_dataset.jsonl`` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e74a3f-4617-4f97-8947-391bd70427ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset):\n",
    " \n",
    "    #Data mapping function\n",
    "    def create_conversation(sample):   \n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are medical professional.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": sample[\"input\"]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": sample[\"output\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    #By default the map() function merges new columns to the dataset.\n",
    "    dataset = dataset.map(\n",
    "        create_conversation, \n",
    "        remove_columns=[\"input\", \"output\", \"instruction\"])\n",
    "\n",
    "    # Save dataset\n",
    "    dataset.to_json(\"train_medical_dataset.jsonl\", orient=\"records\")\n",
    " \n",
    "prepare_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0019cb-074d-4a88-9805-6f508bee8e00",
   "metadata": {},
   "source": [
    "JSONL is an interesting format where each line is a JSON document. Open the ``train_medical_dataset.jsonl`` file and review it.\n",
    "\n",
    "Data conversion needs to be done only once. Before running training we need to load the converted data.\n",
    "\n",
    "## Load the Converted Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e474eab-20bc-43b3-9665-f53e4fbef879",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"train_medical_dataset.jsonl\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3fa8d0-39a5-4ec8-86ab-d7ea5c662796",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2cfeb-7c48-4b0e-b6c0-329981d81303",
   "metadata": {},
   "source": [
    "## Load the Base Model\n",
    "\n",
    "This code will load the base model with 4bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c5e0d-26c0-4d1a-b6d0-c3b0df77f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    #For 4bit quantization\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    ")\n",
    "\n",
    "base_model_name = \"EleutherAI/pythia-1B-deduped\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name)\n",
    "\n",
    "#The base tokenizer does not have a prompt template.\n",
    "#We add it here.\n",
    "tokenizer.chat_template = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831520e7-3770-4d78-a6f3-43c4a4b71434",
   "metadata": {},
   "source": [
    "## Evaluate the Base Model\n",
    "\n",
    "Before running any training we should see if the base model is any good at solving our problems. We write a simple utility to perform text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30182042-9315-4ebc-88b8-ffcfed50f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, question):\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are medical professional.\"},\n",
    "      {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    #This will convert the messages list to text and then tokenize it.\n",
    "    encoded = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\").to(model.device)\n",
    "      \n",
    "    generated_ids = model.generate(encoded, streamer=streamer, max_new_tokens=256)\n",
    "\n",
    "def sample_qa(model, tokenizer, dataset):\n",
    "    dataset = dataset.shuffle()\n",
    "    batch = dataset.select(range(1))\n",
    "    sample = batch[\"messages\"][0]\n",
    "    question = sample[1][\"content\"]\n",
    "    expected_answer = sample[2][\"content\"]\n",
    "    \n",
    "    print(\"Question:\\n\", question, \"\\n\")\n",
    "    print(\"Expected answer:\\n\", expected_answer, \"\\n\")\n",
    "    generate(model, tokenizer, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d0d22-ef1d-4df2-981a-d9b107ecfe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give it a try.\n",
    "sample_qa(base_model, tokenizer, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63048a54-2003-4dfb-9729-002aada0412c",
   "metadata": {},
   "source": [
    "Biggest problem with the model right now is that it doesn't know when to stop answering. Let’s see if fine-tuning will help.\n",
    "\n",
    "## Run Training\n",
    "\n",
    "First we configure the training parameters. We run training for 2 epoch. Each batch will have 5 samples of training data. We set the maximum sequence length to only 300 because we're using a very small language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb811a35-467e-43b5-994f-7f5f061ff0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    " \n",
    "args = SFTConfig(\n",
    "    output_dir=\"medical-trained-model\", # directory to save and repository id\n",
    "    num_train_epochs=2,                     # number of training epochs\n",
    "    per_device_train_batch_size=5,          # batch size per device during training\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=2,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    max_length=300, #Maximum number of generated tokens\n",
    "    packing=True,\n",
    ")\n",
    " \n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81265900-1604-4246-a2cc-3a41bb254a2d",
   "metadata": {},
   "source": [
    "Now, we can begin training. As training progresses you should see a dramatic reduction in loss. This is always a welcome sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b094c-aaf8-42b8-abb0-d83bf050e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aba3c7-f088-463a-955c-96c811d35995",
   "metadata": {},
   "source": [
    "While training is going on, you can use the ``nvidia-smi`` command to check GPU usage and memory avalability.\n",
    "\n",
    "## Save the Model\n",
    "\n",
    "The model weights are saved for every epoch in the ``./chat-trained-model`` folder. But we should save the final version. This will save the model as well as the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d2520-fcbc-4425-99c8-15a993eb46ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97eb05-8651-4887-904d-9e22222d0d6c",
   "metadata": {},
   "source": [
    "Open ``./medical-trained-model/tokenizer_config.json`` to verify that the chat template is now set for the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9256b511-9df8-4279-a532-19689b3bd4af",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "To run inference we need to load the fine-tuned model from the ``./trained-model`` folder. This model is already quantized. There’s no need to quantize it again.\n",
    "\n",
    "Before you go forward I recommend that you restart the notebook session or run this code to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde439ce-1f28-445e-88bf-7c3a12cf50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Free up memory taken up during training\n",
    "del base_model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45105985-746f-496f-bb7f-9f7a7ffb8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"medical-trained-model\",\n",
    "    device_map=\"auto\")\n",
    " \n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"medical-trained-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb88ffe-092e-4324-bf03-0482c87013b9",
   "metadata": {},
   "source": [
    "Run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b9e90-c8cf-47b2-bf58-125442f19944",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_qa(trained_model, trained_tokenizer, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45df40-248a-420a-b9cb-1ee4eb20242c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here we built a proper medical chat model. After training the model gained medical knowledge and the ability to chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b9e1e-7a9a-43db-9514-a21062c20027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
