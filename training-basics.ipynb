{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d11f7b-43d0-4cc6-800f-5a8718538e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad62ed70-fdae-4546-8f5e-8932d6b96600",
   "metadata": {},
   "source": [
    "# Training Causal Language Models\n",
    "Causal language models are transformer models that generate text. These are more popularly known as Large Language Models (LLM).\n",
    "\n",
    "## Progression of Training\n",
    "\n",
    "<img src=\"assets/training-progress.svg\">\n",
    "\n",
    "## How do Language Models Generate Text?\n",
    "Given a prompt text as input, these models output the most probable next word in the sequence. For example:\n",
    "\n",
    "```\n",
    "Input: A dog is a man's best\n",
    "\n",
    "Output probability distribution (softmax) \n",
    "for all words the in vocabulary: [0.0013, 0.0091, ..., 0.034]\n",
    "\n",
    "Most probable word (argmax): friend\n",
    "```\n",
    "\n",
    "<img src=\"assets/probability-distribution.svg\">\n",
    "\n",
    "> To add some creativity to the generation process we can choose a less likely candidate than argmax. This behavior is controlled by a temperature setting.\n",
    "\n",
    "Let's see the process in action. We will use the ``GPT2`` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdfb9c4-b77d-489c-a72a-844d81f1c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf263c26-6513-4f2b-b24c-bf55f6dc009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the text predicted by the model\n",
    "def print_text_predicted(input_text):\n",
    "    input_ids = tokenizer(\n",
    "        input_text, \n",
    "        return_tensors=\"pt\")\n",
    "\n",
    "    #Run a forward pass (inference)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    \n",
    "    print(\"All outputs:\", logits.shape)\n",
    "\n",
    "    #We only use the last probability distribution\n",
    "    last_logit = logits[:, -1, :]\n",
    "    \n",
    "    print(\"Last output:\", last_logit.shape)\n",
    "\n",
    "    #Find the most probable token\n",
    "    predicted_token_id = torch.argmax(last_logit, dim=1)\n",
    "\n",
    "    #Convert token ID to text\n",
    "    predicted_text = tokenizer.decode(predicted_token_id)\n",
    "    \n",
    "    print(\"Next word:\", predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace14ad-f798-4221-9321-ad348cf8494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_text_predicted(\"Miami is a great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269db13-cac4-4cca-b7e7-087f1a1738d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_text_predicted(\"A dog is a man's best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71657c79-ef09-408e-b87b-e2885ef0fc0c",
   "metadata": {},
   "source": [
    "## Text Sequence Generation\n",
    "To generate a continous sequence of text all we have to do is append the predicted token to the list of input tokens and run inference again. In the following code we do exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5f402-95ac-459a-89cf-bf3e27e7ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f978b-4954-4a39-a60e-7da9d6802f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(\"Miami is a great\", \n",
    "          max_new_tokens=20, \n",
    "          #This should pick the most probable next token\n",
    "          temperature=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cbef38-e885-4322-8c0b-2999f7cc4eab",
   "metadata": {},
   "source": [
    "## Text Prediction Training Data\n",
    "Basic text generation models train on a huge body (corpus) of text data. This text data is then broken up into input and target (or, label). \n",
    "\n",
    "Let's say the corpus is like this:\n",
    "\n",
    "```\n",
    "We have just finished a forced march of about forty miles, and have\n",
    "fallen back from near Fredericksburg to within ten miles of Richmond.\n",
    "The Yankees intended to take the Richmond and Potomac Railroad, so we\n",
    "came to reinforce the army already stationed here.\n",
    "```\n",
    "\n",
    "This will be processed into input (x) and target (y) like this.\n",
    "\n",
    "| Input    | Target |\n",
    "| -------- | ------- |\n",
    "| We have just  | finished    |\n",
    "| have just finished | a     |\n",
    "| just finished a    | forced   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007c5597-97cc-4a86-bc74-060fe2070eb6",
   "metadata": {},
   "source": [
    "# Full Training of a Model with American Slang\n",
    "We will now do full training of all the weights of the ``GPT2`` model so that it understands American slang better.\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "We have a small dataset where we have statements made using American slang. Open ``data/slang-talk.jsonl`` and inspect it. You'll see samples like these:\n",
    "\n",
    "```json\n",
    "{\"slang\":\"I'm feeling pretty amped for the concert tonight.\"}\n",
    "{\"slang\":\"That new game is absolutely fire.\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8242fc11-e23e-4189-b94c-fe0c3ee9371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"data/slang-talk.jsonl\",\n",
    "    split=\"train\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b87149e-1c8b-4cdb-b84a-32d7680cfbb6",
   "metadata": {},
   "source": [
    "## Prepare for Training\n",
    "We want to train the GPT2 model that was loaded earlier. ``SFTTrainer`` will automatically deduce the tokenizer for the model and use it to convert the prompt text into token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af311c6c-73e8-4f43-a808-d028ff018dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"slang-gpt2\",\n",
    "    dataset_text_field=\"slang\",\n",
    "    num_train_epochs=2,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e9fbc-b195-473d-a699-93a68bd36667",
   "metadata": {},
   "source": [
    "## Inspect Input Data\n",
    "We can actually observe how the trainer pulls in data from the dataset and creates batches. By default batch size is 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5e802-9852-42f6-ad4b-2d1370b17620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the training dataloader\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "\n",
    "# Iterate to get the first batch\n",
    "first_batch = next(iter(train_dataloader))\n",
    "\n",
    "print(\"Batch shape:\", first_batch['input_ids'].shape)\n",
    "\n",
    "#Decode the input IDs\n",
    "tokenizer.batch_decode(first_batch['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946ca40-7f78-4b52-ba70-b484f9ac1884",
   "metadata": {},
   "source": [
    "## Begin Training\n",
    "We will now train the entire neural network of the model. That is, all the weights will be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d584521-8012-4aae-aee5-ba29ee04f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a15e0e-0faf-4c56-a247-aed7a1d8202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fc2da-b820-4075-9064-0e38f63f1442",
   "metadata": {},
   "source": [
    "## Run Inference Using Trained Model\n",
    "To run inference using the trained model we load it and the tokenizer from the local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61765a5e-f1bb-4baa-b252-4a81c12525ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_gen = pipeline('text-generation', model='slang-gpt2')\n",
    "\n",
    "base_gen = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f32a2b-dd97-416b-be9a-264a4f7697dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_gen(\"Let's yeet\", max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df96480-ce3b-4807-9e83-11d2a6cc2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_gen(\"Let's yeet\", max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e7f5c-6983-474c-8d15-3f1ff27943cc",
   "metadata": {},
   "source": [
    "## Problem with Full Training\n",
    "Full training a modern LLM with a large dataset can cost a lot and take a long time. A more affordable solution is to do partial training using a technique called PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a6e819-ec04-4c96-ac34-3eafbfc161b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
