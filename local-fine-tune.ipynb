{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456b0ba-e623-4f27-b820-a65a6bbd4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37b2cb-4ff6-4ed9-af5e-c89d245b3f7d",
   "metadata": {},
   "source": [
    "# Supervised Fine-tuning of a Hugging Face LLM Model\n",
    "\n",
    "Supervised Fine-tuning (SFT) is used to teach a model new behavior and skills. SFT along with RAG form the two main ways one can customize an LLM. [Newer approaches](https://gorilla.cs.berkeley.edu/blogs/9_raft.html) are also emerging.\n",
    "\n",
    "## When to Fine-tune?\n",
    "\n",
    "Before fine-tuning one should consider RAG. RAG is used mainly to incorporate new knowledge (facts) into an LLM based application. These facts remain in your own documents and never become “learned” by the model. The model can read the facts in your document and reason about them. This is very useful and many applications can be written using just RAG.\n",
    "\n",
    "Fine-tuning teaches new behavior and skills to a model. Consider these use cases:\n",
    "\n",
    "- Read a medical document and answer questions. Medical documents are written in a particular way that an out of the box model may not understand very well.\n",
    "- Convert a plain text question into a SQL query.\n",
    "- Summarize a property lease document. A model by default may not know what the key points are in a lease.\n",
    "\n",
    "In each of these cases RAG may not provide a satisfactory solution. We can, however, teach the model to do these things to our expected level of satisfaction. This is where SFT comes in. We call it “supervised” because during training we provide the question and context and teach the model what a good answer would look like.\n",
    "\n",
    "SFT can also be used to incorporate new knowledge into the model. But that is not its primary purpose. This is mainly due to these reasons:\n",
    "\n",
    "- The facts learned by a model are never quite exact. For example, the Mistral-7B-Instruct model is 15GB in size. Even though it was trained on a huge corpus of text, it cannot possibly contain all the knowledge of the world. That is not how LLMs work. They are not knowledge repositories.\n",
    "- Facts can change from day to day. Retraining a model can be expensive.\n",
    "\n",
    "## Parameter Efficient Fine-tuning (PEFT)\n",
    "\n",
    "Retraining a large model can take a lot of GPU power and time. Fortunately, we’ve discovered that during training we can calculate the gradients and apply corrections to a small subset of the weights and still get very high quality results. This is called Parameter Efficient Fine-tuning (PEFT).\n",
    "\n",
    "Several different approaches to PEFT exist. Adapter, Low-Rank Adaptation (LoRA), and Prefix tuning to name a few. In this notebook we will use LoRA.\n",
    "\n",
    "## Quantization and PEFT\n",
    "\n",
    "The story continues to get even better. [We’ve found out](https://arxiv.org/abs/2305.14314) that quantization can be combined with PEFT and still get high quality results. When quantization is combined with LoRA, we call it QLoRA.\n",
    "\n",
    "In this notebook we will do QLoRA.\n",
    "\n",
    "## Hugging Face Support for PEFT\n",
    "\n",
    "Hugging Face originally released the [PEFT](https://github.com/huggingface/peft) library. Later, [TRL](https://github.com/huggingface/trl) was released that wrapped over the PEFT library and made it even easier to run fine-tuning. They support quantization and work in conjunction with BitsAndBytes.\n",
    "\n",
    "The [SFTTrainer](https://huggingface.co/docs/trl/v0.8.0/en/sft_trainer#trl.SFTTrainer) class forms the heart of the TRL library.\n",
    "\n",
    "In this notebook we will use TRL. These packages are installed as follows.\n",
    "\n",
    "```\n",
    "pip install peft trl\n",
    "```\n",
    "\n",
    "## The Business Problem\n",
    "\n",
    "[Midjourney](https://www.midjourney.com/) is a text to art generator. It requires some skill to come up with effective prompts with special instructions like “35mm film, epic, dramatic, photorealistic, -ar 3:2”. We are being asked to develop an application where users can enter plain English prompts and generate effective Midjourney prompts.\n",
    "\n",
    "An example interaction with LLM will look like this.\n",
    "\n",
    "```\n",
    "User:\n",
    "Generate a prompt for Midjourney based on the sentence below.\n",
    "\n",
    "A blue table against a red wall.\n",
    "\n",
    "LLM:\n",
    "blue table, red wall, photorealistic –ar 4:3 –v 5.1\n",
    "```\n",
    "\n",
    "## Proposed Solution\n",
    "\n",
    "This problem is a classic use case for fine-tuning. Adding knowledge to the system using RAG won’t help very much. We need to teach the model a new skill to translate plain English text to Midjourney prompt.\n",
    "\n",
    "We choose to use a few techniques to speed up training and inference:\n",
    "\n",
    "- Use a small language model and see if that works well. We will use TinyLlama/TinyLlama-1.1B-Chat-v1.0.\n",
    "- Use LoRA PEFT.\n",
    "- Use 4bit quantization.\n",
    "\n",
    "We will use the [TheBossLevel123/midjourney-prompt-enhancement](https://huggingface.co/datasets/TheBossLevel123/midjourney-prompt-enhancement) dataset for training. It gives us exactly what we need – translation between plain text and good quality Midjourney prompts.\n",
    "\n",
    "Go to the dataset's home page and review what it looks like.\n",
    "\n",
    "## Prepare Training Data\n",
    "\n",
    "SFTTrainer supports [two different data formats](https://huggingface.co/docs/trl/en/sft_trainer#dataset-format-support). We will format each piece of training data as follows.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"Generate a prompt for Midjourney based on the sentence below\"},\n",
    "    {\"role\": \"user\", \"content\": \"A bustling city street scene at night.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Photorealistic bustling city street at night, vibrant lights, busy pedestrians, urban life --ar 16:9 --v 5.2\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "The source dataset has two columns:\n",
    "\n",
    "- input – The plain text prompt\n",
    "- output – Good quality Midjourney prompt\n",
    "\n",
    "The code below will load the dataset, reformat it according to the requirement of SFTTrainer and save it in the ``train_dataset.jsonl`` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e74a3f-4617-4f97-8947-391bd70427ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    " \n",
    "    #Data mapping function\n",
    "    def create_conversation(sample):\n",
    "        return {\n",
    "          \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"Generate a prompt for Midjourney based on the sentence below\"},\n",
    "            {\"role\": \"user\", \"content\": sample[\"input\"]},\n",
    "            {\"role\": \"assistant\", \"content\": sample[\"output\"]}\n",
    "          ]\n",
    "        }\n",
    " \n",
    "    dataset = load_dataset(\n",
    "        \"TheBossLevel123/midjourney-prompt-enhancement\", \n",
    "        split=\"train\")\n",
    "     \n",
    "    #By default the map() function merges new columns to the dataset.\n",
    "    #We need only the \"messages\" column. So, delete the input and output columns.\n",
    "    dataset = dataset.map(\n",
    "        create_conversation, \n",
    "        remove_columns=[\"input\", \"output\"])\n",
    "     \n",
    "    # Save dataset\n",
    "    dataset.to_json(\"train_dataset.jsonl\", orient=\"records\")\n",
    " \n",
    "#Run data conversion\n",
    "prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0019cb-074d-4a88-9805-6f508bee8e00",
   "metadata": {},
   "source": [
    "JSONL is an interesting format where each line is a JSON document. Open the ``train_dataset.jsonl`` file and review it.\n",
    "\n",
    "Data conversion needs to be done only once. Before running training we need to load the converted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e474eab-20bc-43b3-9665-f53e4fbef879",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\n",
    "    \"json\", \n",
    "    data_files=\"train_dataset.jsonl\", \n",
    "    split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2cfeb-7c48-4b0e-b6c0-329981d81303",
   "metadata": {},
   "source": [
    "## Load the Base Model\n",
    "\n",
    "This code will load the base model with 4bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c5e0d-26c0-4d1a-b6d0-c3b0df77f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    #For 4bit quantization\n",
    "    load_in_4bit=True\n",
    ")\n",
    " \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    quantization_config=bnb_config)\n",
    " \n",
    "# Do not do this:\n",
    "# model.to(device)\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "#Some documents ask you to call setup_chat_format().\n",
    "#Don't do this. Doing so will overwrite the correct\n",
    "#tokenizer settings with chatml settings.\n",
    "# model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831520e7-3770-4d78-a6f3-43c4a4b71434",
   "metadata": {},
   "source": [
    "## Evaluate the Base Model\n",
    "\n",
    "Before running any training we should see if the base model is any good at solving our problems. We write a simple utility to perform text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30182042-9315-4ebc-88b8-ffcfed50f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate midjourney prompt\n",
    "def generate(model, tokenizer, prompt):\n",
    "  streamer = TextStreamer(tokenizer)\n",
    "   \n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"Generate a prompt for Midjourney based on the sentence below.\"},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    " \n",
    "  device = \"cuda\"\n",
    " \n",
    "  encoded = tokenizer.apply_chat_template(\n",
    "      messages, \n",
    "      add_generation_prompt=True, \n",
    "      return_tensors=\"pt\").to(device)\n",
    " \n",
    "  generated_ids = model.generate(encoded, streamer=streamer, max_new_tokens=2000)\n",
    "\n",
    "\n",
    "#Give it a try.\n",
    "generate(model, tokenizer, \"A blue table against a red wall.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63048a54-2003-4dfb-9729-002aada0412c",
   "metadata": {},
   "source": [
    "This outputs something not very usable as a Midjourney prompt.\n",
    "\n",
    "```\n",
    "Amidst a sea of red, a blue table stands tall, \n",
    "a symbol of hope and unity.\n",
    "```\n",
    "\n",
    "Clearly, the base model is not very good at this. Let’s see if fine-tuning will help.\n",
    "\n",
    "## Run Training\n",
    "\n",
    "First we configure the training parameters. We run training for 10 epochs. Each batch will have 3 samples of training data. We set the maximum sequence length to only 2000 because we're using a very small language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb811a35-467e-43b5-994f-7f5f061ff0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    " \n",
    "args = TrainingArguments(\n",
    "    output_dir=\"trained-model\", # directory to save and repository id\n",
    "    num_train_epochs=6,                     # number of training epochs\n",
    "    per_device_train_batch_size=3,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=2,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    bf16=False,                              # use bfloat16 precision\n",
    "    tf32=False,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    ")\n",
    " \n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2000,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81265900-1604-4246-a2cc-3a41bb254a2d",
   "metadata": {},
   "source": [
    "Now, we can begin training. As training progresses you should see a dramatic reduction in loss. This is always a welcome sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b094c-aaf8-42b8-abb0-d83bf050e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aba3c7-f088-463a-955c-96c811d35995",
   "metadata": {},
   "source": [
    "While training is going on, you can use the ``nvidia-smi`` command to check GPU usage and memory avalability.\n",
    "\n",
    "## Save the Model\n",
    "\n",
    "The model weights are saved for every epoch in the ./trained-model folder. But we should save the final version. This will save the model as well as the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d2520-fcbc-4425-99c8-15a993eb46ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ba34a-061b-4a35-9ee6-d5a6c7a4c02d",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "We can use the same prompt that we tried before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b3941-00e7-4b52-801d-14a4b9840109",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model, tokenizer, \"A blue table against a red wall.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9256b511-9df8-4279-a532-19689b3bd4af",
   "metadata": {},
   "source": [
    "It will now generate this.\n",
    "\n",
    "```\n",
    "blue table, red wall, photorealistic --ar 4:3 --v 5.1\n",
    "```\n",
    "\n",
    "The model now understands how to generate Midjourney prompts.\n",
    "\n",
    "## Run Inference\n",
    "\n",
    "To run inference we need to load the fine-tuned model from the ``./trained-model`` folder. This model is already quantized. There’s no need to quantize it again.\n",
    "\n",
    "Before you go forward I recommend that you restart the notebook session or run this code to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde439ce-1f28-445e-88bf-7c3a12cf50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Free up memory taken up during training\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45105985-746f-496f-bb7f-9f7a7ffb8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./trained-model\")\n",
    "\n",
    "#You need to load it into the GPU\n",
    "device = \"cuda\"\n",
    " \n",
    "model.to(device)\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./trained-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb88ffe-092e-4324-bf03-0482c87013b9",
   "metadata": {},
   "source": [
    "Run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acc5e0f-0af7-4d39-b1b5-009f6bc01c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model, tokenizer, \"A blue table against a red wall.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45df40-248a-420a-b9cb-1ee4eb20242c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Fine-tuning teaches new skills to a model. It takes more GPU power than RAG but certain things simply cannot be done using RAG. Over the last few months fine-tuning has become more democratized. We’re able to re-train a model in regular GPU using techniques like quantization and PEFT. It’s amazing what even a small LLM can do once it is fine-tuned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
