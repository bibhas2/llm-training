{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d04545e-3536-44e1-a31d-03fe2f5b314d",
   "metadata": {},
   "source": [
    "# Running LLM Locally Using Hugging Face\n",
    "\n",
    "Running a LLM locally has a few benefits:\n",
    "\n",
    "- It may be cheaper\n",
    "- Better data privacy\n",
    "- You have full control over fine-tuning and quantization\n",
    "\n",
    "In this notebook we will learn to run models locally using the Hugging Face ``transformers`` package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7196f-da3f-41ad-9427-469582554c2f",
   "metadata": {},
   "source": [
    "Hugging Face is a repository of open source LLMs. The Git repos are structured in a consistent manner as per Hugging Face specififcations. This makes it easy to download and use these models in a consistent manner. We use the ``transformers`` Python package for this.\n",
    "\n",
    "We will now run the same tinyllama model but using Hugging Face. The model ID is ``\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"``. This translates to the model's home page ``https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0``. You can see a list of all available models [here](https://huggingface.co/models).\n",
    "\n",
    "## Download and Load the Model\n",
    "The ``AutoModelForCausalLM`` Python class is used to download a causal (text generating) language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc4878-2ba6-40b6-9b11-3c875c21f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368282bd-7fd2-4c9f-a614-da81f5a986e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The device to load the model onto. \n",
    "#\n",
    "# Available device types:\n",
    "# \"cuda\" - NVIDIA GPU\n",
    "# \"cpu\" - Plain CPU\n",
    "# \"mps\" - Apple silicon\n",
    "device = \"cuda\"\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# This model requires 24GB GPU memory and 32GB RAM.\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "#Load the model into GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(device)\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1461f2d1-32b5-4fd9-8c54-31c4b1626d5b",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "After the model is loaded it will be cached in the ``~/.cache/huggingface`` folder. We can then run inference. Note: Both the model and the sentence tokens are loaded into the GPU. Also, the sentence tokens are obtained as Pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba296a1-183a-4f35-89f6-a3e799e6094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer)\n",
    " \n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Bob is taller than Jane. Jane is taller than Kim. Is Bob taller than Kim?\"}\n",
    "]\n",
    " \n",
    "encoded = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    add_generation_prompt=True, \n",
    "    return_tensors=\"pt\").to(device)\n",
    " \n",
    "generated_ids = model.generate(encoded, streamer=streamer, max_new_tokens=4096, temperature=0.36)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc930847-7752-4b1a-8543-d8d5a4c97a8a",
   "metadata": {},
   "source": [
    "The answer is obviously wrong. This is common for small models where the reasoning capability is limited. We can fix the problem by improving its reasoning power using Chain of Thought prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba39ccd-8300-4e70-aa44-af277955549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {\"role\": \"user\", \"content\": \n",
    "\"\"\"\n",
    "Tony is taller than Jane. Jane is taller than Agatha. Is Tony taller than Agatha?\n",
    "\"\"\"},\n",
    "    {\"role\": \"assistant\", \"content\": \n",
    "\"\"\"\n",
    "If Jane is taller than Agatha and Tony is taller than Jane then it follows that Tony is also taller than Agatha. So, the\n",
    "answer is yes, Tony is taller than Agatha.\n",
    "\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"Bob is taller than Jane. Jane is taller than Kim. Is Bob taller than Kim?\"}\n",
    "]\n",
    " \n",
    "encoded = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    add_generation_prompt=True, \n",
    "    return_tensors=\"pt\").to(device)\n",
    " \n",
    "generated_ids = model.generate(encoded, streamer=streamer, max_new_tokens=4096, temperature=0.36)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcfbf34-c239-46cc-9912-2c3a81291982",
   "metadata": {},
   "source": [
    "## Shared Device Usage\n",
    "In the example above we load the entire model into the GPU if available. This approach will fail if you try to load a large model and your GPU doesn't have enough VRAM. A better way to load a model is to fit the model into the GPU as much as possible and if necessary use SRAM and disk for the remaining. This way, things will be slow but at least it will work.\n",
    "\n",
    "In the example below we load the ``EleutherAI/pythia-70m-deduped`` model by automatically sharing the memory of GPU, SRAM and disk. This is a base model and can only generate text but cannot do question/answer style chatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98835129-763e-4c85-a43b-5fd55c064e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unload the previous model\n",
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf0e57-839d-4ee9-bb4c-89ddc2c2f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "            torch_dtype=torch.float16, \n",
    "            #This will load model into cuda first\n",
    "            #and then use SRAM if more space is needed.\n",
    "            device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb6257-4fe9-446f-b378-c74579e69194",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"A dog is a man's best\", return_tensors=\"pt\")\n",
    "\n",
    "#Load the tokens into the same device as the model\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "tokens = model.generate(**inputs)\n",
    "outputs = tokenizer.decode(tokens[0])\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d56788-ba7f-4a54-8734-915ef5b893c1",
   "metadata": {},
   "source": [
    "## Using Flash Attention\n",
    "Flash attention is a mechanism that speeds up languaage models. It does that by optimizing data transfer from the computer's SRAM to the GPU's VRAM. Flash attention works only with newer versions of CUDA. Also, Windows isn't supported as of June 2025.\n",
    "\n",
    "We install flash attention this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e2cd1-3d68-4457-8c0d-87ffb3440132",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29818ad7-0cfc-4850-b249-9255840aab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "            torch_dtype=torch.float16, \n",
    "            device_map=\"auto\", \n",
    "            #Enable flash attention\n",
    "            attn_implementation=\"flash_attention_2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39c3b0b-6a1d-4e55-8869-f7b461f31a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"A dog is a man's best\", return_tensors=\"pt\")\n",
    "\n",
    "#Load the tokens into the same device as the model\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "tokens = model.generate(**inputs)\n",
    "outputs = tokenizer.decode(tokens[0])\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a43151-93dd-4907-8f9b-44f49a3c8fdd",
   "metadata": {},
   "source": [
    "# Running Smaller Transformer Models\n",
    "Many tasks can be done by small transformer models cheaper and faster than an LLM. Below we use a fine-tuned version of Google's BERT model for sentiment classififcation. Note that, here we use the ``AutoModelForSequenceClassification`` class instead of ``AutoModelForCausalLM``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976ff04-05cf-460f-b37b-40e5af687a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "#Run inference\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "#Get the predicted class with the highest probability\n",
    "predicted_class_id = logits.argmax().item()\n",
    "\n",
    "#Convert the class ID to plain English label\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3963f09d-c95c-4bee-9196-5cff12b62607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
